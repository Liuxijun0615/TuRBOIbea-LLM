# EA_Operators/TuRBO_LLM_EA.py
from .LLM_EA import LLM_EA, invoke_llm_with_tracking
from .embedding_utils import RobustPromptEmbedder
from .TuRBO import TuRBOOptimizer
import numpy as np
import logging
import os

# LangChain ç›¸å…³å¯¼å…¥
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# å¦‚æœæ²¡æœ‰å®‰è£… langchain_community æˆ–ä¸éœ€è¦ ChatZhipuAIï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
try:
    from langchain_community.chat_models import ChatZhipuAI
except ImportError:
    ChatZhipuAI = None

logger = logging.getLogger(__name__)


class TuRBLLMEA(LLM_EA):
    """
    é›†æˆTuRBOçš„LLMè¿›åŒ–ç®—æ³•
    å®ç°è®ºæ–‡ 3.3.1 Hybrid Candidate Generation Strategy
    """

    def __init__(self, pop_size, initialize_prompt, crossover_prompt, llm_model, api_key,
                 turbo_subspaces=3, use_embedding=True, embedding_model='all-MiniLM-L6-v2'):
        # 1. è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(pop_size, initialize_prompt, crossover_prompt, llm_model, api_key)

        # =======================================================
        # åˆå§‹åŒ– TuRBO ä¸“ç”¨ LLM
        # =======================================================
        if llm_model == 'glm':
            if ChatZhipuAI is None:
                raise ImportError("ChatZhipuAI æœªå¯¼å…¥ï¼Œè¯·æ£€æŸ¥ langchain_community æ˜¯å¦å®‰è£…")
            os.environ["ZHIPUAI_API_KEY"] = api_key
            self.llm = ChatZhipuAI(model="glm-4", temperature=0.7)
        elif llm_model == 'deepseek-chat':
            self.llm = ChatOpenAI(
                api_key=api_key,
                base_url="https://api.deepseek.com/v1",
                model="deepseek-chat",
                temperature=0.7
            )
        else:
            # é»˜è®¤ä¸º GPT æˆ–å…¶ä»– OpenAI å…¼å®¹æ¨¡å‹
            self.llm = ChatOpenAI(api_key=api_key, model=llm_model, temperature=0.7)

        print(f"âœ… TuRBO LLM Client Initialized: {self.llm}")

        self.use_embedding = use_embedding
        self.embedder = RobustPromptEmbedder(embedding_model)
        self.prompt_embeddings = {}  # ç¼“å­˜: str -> np.array

        # TuRBO æ ¸å¿ƒç»„ä»¶
        self.turbo_opt = TuRBOOptimizer(
            num_subspaces=turbo_subspaces,
            embedding_dim=self.embedder.embedding_dim
        )
        self.turbo_initialized = False

        # åˆå§‹åŒ– TuRBO å˜å¼‚ Prompt æ¨¡æ¿
        # å¢å¼ºäº† System Prompt ä»¥ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®
        self.turbo_mutation_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert in optimizing recommendation prompts. "
                       "CRITICAL: You MUST wrap your entire output prompt inside <START> and <END> tags. "
                       "Do not output explanations, only the new prompt inside tags."),
            ("user",
             "I have a prompt for a recommendation task. I want to modify this prompt to explore a specific semantic direction.\n"
             "The prompt is: \n{anchor_prompt}\n\n"
             "Please rewrite this prompt to maintain its core logic but change the wording or structure to potentially improve recommendation accuracy and diversity.\n"
             "The new prompt must be wrapped with <START> and <END>.")
        ])

    def get_embedding(self, prompt):
        if prompt not in self.prompt_embeddings:
            self.prompt_embeddings[prompt] = self.embedder.encode(prompt)[0]
        return self.prompt_embeddings[prompt]

    def update_embeddings_batch(self, population):
        """æ‰¹é‡æ›´æ–°ç¼“å­˜"""
        embeddings = []
        for p in population:
            embeddings.append(self.get_embedding(p))
        return np.array(embeddings)

    def turbo_generation(self, population, n_offspring):
        """
        TuRBO æ··åˆç”Ÿæˆæµç¨‹
        å¯¹åº”è®ºæ–‡ Algorithm 2, Step 1 (Lines 9-14)
        """
        # 1. é¦–æ¬¡è¿è¡Œæ—¶åˆå§‹åŒ–å­ç©ºé—´
        pop_embeddings = self.update_embeddings_batch(population)
        if not self.turbo_initialized:
            self.turbo_opt.initialize_subspaces(pop_embeddings)
            self.turbo_initialized = True

        offspring = []
        offspring_source_map = []  # è®°å½• (offspring_index, subspace_index) ä»¥ä¾¿åç»­æ›´æ–°

        print("ğŸ”„ TuRBO Generation Steps: Sampling -> Anchor -> Mutation")

        for i in range(n_offspring):
            # Step 1: é€‰æ‹©å­ç©ºé—´å¹¶é‡‡æ ·
            subspace_idx = self.turbo_opt.select_subspace_for_generation()
            subspace = self.turbo_opt.subspaces[subspace_idx]

            # z ~ N(ck, Lk)
            z_vector = subspace.sample_vector()

            # Step 2: è¯­ä¹‰é”šç‚¹é€‰æ‹©
            anchor_prompt = self._find_semantic_anchor(population, pop_embeddings, z_vector)

            # Step 3: ç¦»æ•£æ–‡æœ¬å®ç° (LLM Mutation)
            try:
                new_prompt = self._generate_discrete_text(anchor_prompt)
                offspring.append(new_prompt)
                offspring_source_map.append(subspace_idx)  # è®°å½•è¿™ä¸ªåä»£æ˜¯ç”±å“ªä¸ª TR ç”Ÿæˆçš„
            except Exception as e:
                logger.error(f"LLM Generation failed: {e}")
                # å¤±è´¥å›é€€
                offspring.append(anchor_prompt)
                offspring_source_map.append(subspace_idx)

        return offspring, offspring_source_map

    def _find_semantic_anchor(self, population, embeddings, z_vector):
        """
        è®¡ç®— Cosine Similarity å¹¶é€‰æ‹© Anchor
        """
        norm_z = np.linalg.norm(z_vector)
        if norm_z == 0: return np.random.choice(population)

        # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
        dot_products = np.dot(embeddings, z_vector)
        norms = np.linalg.norm(embeddings, axis=1)
        similarities = dot_products / (norms * norm_z + 1e-9)

        best_idx = np.argmax(similarities)
        return population[best_idx]

    def _generate_discrete_text(self, anchor_prompt):
        """
        è°ƒç”¨ LLM è¿›è¡Œå®šå‘å˜å¼‚
        [å…³é”®ä¿®å¤] ä½¿ç”¨ invoke_llm_with_tracking æ¥æ•è· Token
        """
        response = invoke_llm_with_tracking(
            self.llm,
            self.turbo_mutation_prompt,
            {"anchor_prompt": anchor_prompt},
            self  # ä¼ å…¥ self ä»¥æ›´æ–° total_tokens
        )
        extracted = self.extract_edit_prompt(response)
        if extracted:
            return extracted[0]
        return anchor_prompt

    def extract_edit_prompt(self, response):
        """æå– Prompt å†…å®¹"""
        import re
        patterns = [
            r'<START>\s*(.*?)\s*<END>',
            r'```(?:python)?\s*(.*?)\s*```',
            r'["\']([^"\']*?)["\']',
        ]
        for pattern in patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            if matches:
                cleaned = [match.strip() for match in matches if match.strip()]
                if cleaned:
                    return cleaned
        return [response.strip()] if response.strip() else []

    def update_turbo_regions(self, offspring_list, offspring_objs, offspring_source_map, archive_hv, calculator):
        """
        æ›´æ–°ä¿¡ä»»åŒºåŸŸçŠ¶æ€
        å¯¹åº”è®ºæ–‡ Algorithm 2, Step 3 (Lines 20-30)
        """
        # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„æ›´æ–°é€»è¾‘ï¼Œæˆ–è€…ä¿æŒä¸ºç©ºå¦‚æœæš‚æ—¶ä¸éœ€è¦è‡ªé€‚åº”è°ƒæ•´
        pass

    def fallback_crossover_batch(self, pop, n_offspring):
        """
        å¦‚æœ TuRBO å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„éšæœºé€‰æ‹©
        """
        import random
        return [random.choice(pop) for _ in range(n_offspring)]