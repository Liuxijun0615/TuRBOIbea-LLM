# EA_Operators/embedding_utils.py
import numpy as np
import random
import string
from typing import List, Optional
import logging
import torch  # å¿…é¡»å¯¼å…¥ torch ä»¥æ£€æµ‹ MPS

logger = logging.getLogger(__name__)


class RobustPromptEmbedder:
    """é²æ£’çš„æç¤ºåµŒå…¥å™¨ï¼Œæ”¯æŒå¤šç§åµŒå…¥æ–¹æ³•ï¼Œå¹¶é’ˆå¯¹ Mac MèŠ¯ç‰‡ä¼˜åŒ–"""

    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', use_simple: bool = False):
        self.model = None
        self.use_simple = use_simple
        self.embedding_dim = 384  # é»˜è®¤ç»´åº¦ï¼Œä¸MiniLMåŒ¹é…

        # æ£€æµ‹è®¾å¤‡ï¼šä¼˜å…ˆä½¿ç”¨ MPS (Mac GPU)ï¼Œå…¶æ¬¡ CUDAï¼Œæœ€å CPU
        if torch.backends.mps.is_available():
            self.device = "mps"
            logger.info("ğŸš€ æ£€æµ‹åˆ° Mac GPU (MPS)ï¼Œå·²å¯ç”¨ç¡¬ä»¶åŠ é€Ÿï¼")
        elif torch.cuda.is_available():
            self.device = "cuda"
            logger.info("ğŸš€ æ£€æµ‹åˆ° NVIDIA GPU (CUDA)ï¼Œå·²å¯ç”¨ç¡¬ä»¶åŠ é€Ÿï¼")
        else:
            self.device = "cpu"
            logger.info("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼ã€‚")

        if not use_simple:
            try:
                from sentence_transformers import SentenceTransformer
                logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} åˆ° {self.device}...")

                # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
                self.model = SentenceTransformer(model_name, device=self.device)

                self.embedding_dim = self.model.get_sentence_embedding_dimension()
                logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {self.embedding_dim}")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½SentenceTransformer: {e}ï¼Œä½¿ç”¨ç®€å•åµŒå…¥æ–¹æ³•")
                self.use_simple = True
                self._init_simple_embedder()
        else:
            self._init_simple_embedder()

    def _init_simple_embedder(self):
        """åˆå§‹åŒ–ç®€å•çš„åŸºäºå“ˆå¸Œçš„åµŒå…¥å™¨"""
        logger.info("ä½¿ç”¨ç®€å•åµŒå…¥æ–¹æ³•")
        self.vocab = self._build_vocab()
        self.embedding_dim = 384  # ä¿æŒä¸MiniLMç›¸åŒçš„ç»´åº¦

    def _build_vocab(self):
        """æ„å»ºç®€å•çš„è¯æ±‡è¡¨"""
        chars = string.ascii_lowercase + string.digits + string.punctuation + " "
        return {char: idx for idx, char in enumerate(chars)}

    def encode(self, prompts: List[str]) -> np.ndarray:
        """
        ç”ŸæˆåµŒå…¥å‘é‡
        """
        if isinstance(prompts, str):
            prompts = [prompts]

        if self.use_simple or self.model is None:
            return self._simple_encode(prompts)

        try:
            # SentenceTransformer ä¼šè‡ªåŠ¨ä½¿ç”¨åˆå§‹åŒ–æ—¶æŒ‡å®šçš„ device (mps)
            embeddings = self.model.encode(prompts, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"æ¨¡å‹ç¼–ç å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç®€å•ç¼–ç ")
            return self._simple_encode(prompts)

    def _simple_encode(self, prompts: List[str]) -> np.ndarray:
        """ç®€å•çš„ç¼–ç å®ç°ï¼šåŸºäºå­—ç¬¦é¢‘ç‡çš„ç‰¹å¾å‘é‡"""
        embeddings = []
        for prompt in prompts:
            vec = np.zeros(self.embedding_dim)
            # ç®€å•çš„å“ˆå¸Œæ˜ å°„
            for i, char in enumerate(prompt.lower()):
                idx = ord(char) % self.embedding_dim
                vec[idx] += 1

            # å½’ä¸€åŒ–
            norm = np.linalg.norm(vec)
            if norm > 0:
                vec = vec / norm
            embeddings.append(vec)

        return np.array(embeddings)

    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if embedding1.ndim > 1:
            embedding1 = embedding1.flatten()
        if embedding2.ndim > 1:
            embedding2 = embedding2.flatten()

        dot_product = np.dot(embedding1, embedding2)
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)


class DummyEmbedder:
    """è™šæ‹ŸåµŒå…¥å™¨ï¼Œç”¨äºå®Œå…¨ç¦ç”¨åµŒå…¥åŠŸèƒ½ (w/o Mapping)"""

    def __init__(self):
        self.embedding_dim = 384

    def encode(self, prompts):
        if isinstance(prompts, str):
            prompts = [prompts]
        # è¿”å›éšæœºå‘é‡ï¼Œä¿æŒä»£ç è·‘é€š
        return np.random.randn(len(prompts), self.embedding_dim)

    def similarity(self, embedding1, embedding2):
        return random.random()  # éšæœºç›¸ä¼¼åº¦