from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from .utils import extract_edit_prompt, environment_selection, Tchebycheff, choice_matrix, IBEA_Selection
import json
import numpy as np
import random
import os
import time


def invoke_llm_with_tracking(llm, prompt_template, input_variables, tracker_obj):
    messages = prompt_template.format_messages(**input_variables)
    response = llm.invoke(messages)
    tracker_obj.total_api_calls += 1
    usage = response.response_metadata.get('token_usage', {})
    tokens = usage.get('total_tokens', 0)
    if tokens > 0:
        tracker_obj.total_tokens += tokens
    else:
        input_text = str(input_variables)
        output_text = response.content
        estimated = (len(input_text) + len(output_text)) // 4
        tracker_obj.total_tokens += estimated
    return response.content


class LLM_EA():
    def __init__(self, pop_size, initialize_prompt, crossover_prompt, llm_model, api_key):
        self.pop_size = pop_size
        self.total_api_calls = 0
        self.failed_api_calls = 0
        self.total_tokens = 0
        self.start_time = None

        # åˆå§‹åŒ– LLM
        if llm_model == 'glm':
            os.environ["ZHIPUAI_API_KEY"] = api_key
            self.llm_initialize = ChatZhipuAI(model="glm-4", api_key=api_key, temperature=0.7)
            self.llm_operator = ChatZhipuAI(model="glm-4", api_key=api_key, temperature=0.7)
        elif llm_model == 'deepseek-chat':
            self.llm_initialize = ChatOpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1",
                                             model="deepseek-chat", temperature=0.7)
            self.llm_operator = ChatOpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1",
                                           model="deepseek-chat", temperature=0.7)
        else:
            self.llm_initialize = ChatOpenAI(api_key=api_key, model=llm_model)
            self.llm_operator = ChatOpenAI(api_key=api_key, model=llm_model)

        # [å…³é”®ä¿®æ”¹] å¢å¼º System Promptï¼Œé˜²æ­¢ç”Ÿæˆç©ºå†…å®¹
        system_instruction = (
            "You are an evolutionary operator for prompt optimization. "
            "Your task is to generate a new, improved prompt based on the input. "
            "CRITICAL: You MUST wrap your entire output prompt inside <START> and <END> tags. "
            "Do not output explanations, only the new prompt inside tags."
        )

        self.prompt_initialize = ChatPromptTemplate.from_messages([
            ("system", "You are an initializer. " + system_instruction),
            ("user", initialize_prompt)]
        )

        self.prompt_operator = ChatPromptTemplate.from_messages([
            ("system", "You are a crossover operator. " + system_instruction),
            ("user", crossover_prompt)]
        )

    # ... (initialize, crossover, naive, environment_selection, IBEA_selection ç­‰æ–¹æ³•ä¿æŒä¹‹å‰çš„ä¿®å¤ç‰ˆé€»è¾‘)
    # ä¸ºèŠ‚çœç¯‡å¹…ï¼Œè¯·ç¡®ä¿è¿™é‡Œä¿ç•™äº†ä¹‹å‰æä¾›çš„ `invoke_llm_with_tracking` è°ƒç”¨é€»è¾‘
    # æ ¸å¿ƒæ˜¯ä¸Šé¢çš„ System Prompt ä¿®æ”¹

    def initialize(self, example):
        pop = []
        self.start_time = time.time()
        print(f"ğŸš€ å¼€å§‹åˆå§‹åŒ–ç§ç¾¤ï¼Œéœ€è¦ç”Ÿæˆ {self.pop_size} ä¸ªä¸ªä½“")
        for i in range(self.pop_size):
            start_call = time.time()
            while True:
                try:
                    output = invoke_llm_with_tracking(self.llm_initialize, self.prompt_initialize, {"example": example},
                                                      self)
                    call_time = time.time() - start_call
                    print(f"âœ… åˆå§‹åŒ–ä¸ªä½“ {i + 1}/{self.pop_size} å®Œæˆ | APIè°ƒç”¨æ—¶é—´: {call_time:.1f}ç§’")
                    break
                except Exception as e:
                    self.failed_api_calls += 1
                    time.sleep(5)
            individual = extract_edit_prompt(output)
            pop.extend(individual)
        return pop

    def crossover(self, pop):
        offsprings = []
        print(f"ğŸ§¬ å¼€å§‹äº¤å‰å˜å¼‚ï¼Œéœ€è¦ç”Ÿæˆ {self.pop_size} ä¸ªåä»£")
        for i in range(self.pop_size):
            start_call = time.time()
            idx = np.random.choice(len(pop), 2, replace=False)
            while True:
                try:
                    output = invoke_llm_with_tracking(self.llm_operator, self.prompt_operator,
                                                      {"prompt1": pop[idx[0]], "prompt2": pop[idx[1]]}, self)
                    print(f"âœ… åä»£ {i + 1}/{self.pop_size} ç”Ÿæˆå®Œæˆ")
                    break
                except Exception as e:
                    self.failed_api_calls += 1
                    time.sleep(5)
            offspring = extract_edit_prompt(output)
            offsprings.extend(offspring)
        return offsprings

    def enviromnent_selection(self, pop, y_pop, offspring, y_offspring):
        pop.extend(offspring)
        y_pop = np.concatenate((y_pop, y_offspring))
        pop_next, _, _, _ = environment_selection([pop, y_pop], self.pop_size)
        return pop_next[0], pop_next[1]

    def IBEA_selection(self, pop, y_pop, offspring, y_offspring):
        pop.extend(offspring)
        y_pop = np.concatenate((y_pop, y_offspring), axis=0)
        pop, y_pop = IBEA_Selection(pop, y_pop, self.pop_size, 0.05)
        return pop, y_pop


class LLM_MOEAD(LLM_EA):
    # MOEAD ç±»å¯ä»¥ç»§æ‰¿ LLM_EAï¼Œåªéœ€é‡å†™ init å’Œ evolution
    # è¯·ç¡®ä¿å¼•å…¥ä¹‹å‰æä¾›çš„ MOEAD ä¿®å¤é€»è¾‘ï¼ˆé‚»åŸŸä¿®æ­£ç­‰ï¼‰
    def __init__(self, pop_size, obj_num, initialize_prompt, crossover_prompt, weight, num_sub_set, llm_model, api_key):
        super().__init__(pop_size, initialize_prompt, crossover_prompt, llm_model, api_key)
        self.weight = weight
        self.obj_num = obj_num

        # é‚»åŸŸä¿®æ­£
        self.num_sub_set = min(num_sub_set, pop_size)

        from scipy.spatial.distance import cdist
        w_repeat1 = weight.reshape(1, self.pop_size, self.obj_num).repeat(self.pop_size, axis=0)
        w_repeat2 = weight.reshape(self.pop_size, 1, self.obj_num).repeat(self.pop_size, axis=1)
        dist = np.sqrt(np.sum((w_repeat1 - w_repeat2) ** 2, axis=2))
        self.B = np.argsort(dist, axis=1)[:, 0:self.num_sub_set]
        self.p_sel = np.ones([pop_size, self.num_sub_set]) / self.num_sub_set

    def evolution(self, pop, y_pop, obj_func):
        idx_choice = np.random.choice(self.pop_size, self.pop_size, replace=False)
        idx_sel = choice_matrix(self.p_sel, 2)
        # ä¿®æ­£ç´¢å¼•èŒƒå›´
        max_idx = self.B.shape[1] - 1
        w_rand1 = self.B[idx_choice, np.clip(idx_sel[0, idx_choice], 0, max_idx)]
        w_rand2 = self.B[idx_choice, np.clip(idx_sel[1, idx_choice], 0, max_idx)]

        for i in range(self.pop_size):
            parent1 = pop[w_rand1[i]]
            parent2 = pop[w_rand2[i]]

            # ä½¿ç”¨çˆ¶ç±»çš„ crossover é€»è¾‘ç”Ÿæˆå•ä¸ªåä»£
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è°ƒçˆ¶ç±» crossover å¯èƒ½ä¼šç”Ÿæˆä¸€æ‰¹ï¼ŒMOEAD åªéœ€è¦ä¸€ä¸ª
            # å»ºè®®åœ¨ LLM_EA ä¸­å¢åŠ  crossover_single æ–¹æ³•ï¼Œæˆ–è€…åœ¨è¿™é‡Œæ‰‹åŠ¨è°ƒç”¨
            try:
                output = invoke_llm_with_tracking(self.llm_operator, self.prompt_operator,
                                                  {"prompt1": parent1, "prompt2": parent2}, self)
                offspring = extract_edit_prompt(output)
            except:
                offspring = [parent1]

            if not offspring: offspring = [parent1]

            y_offspring = obj_func(offspring)
            if len(y_offspring.shape) == 1: y_offspring = y_offspring.reshape(1, -1)

            # Tchebycheff æ›´æ–°é€»è¾‘ (åŒä¹‹å‰)
            z_min = np.min(np.vstack((y_pop, y_offspring)), axis=0)
            y_pop_tch = Tchebycheff(y_pop[self.B[idx_choice[i]]], self.weight[self.B[idx_choice[i]]])
            y_offspring_tch = Tchebycheff(y_offspring, self.weight[self.B[idx_choice[i]]])
            idx_update = self.B[idx_choice[i], y_offspring_tch < y_pop_tch]
            for idx in idx_update:
                pop[idx] = offspring[0]
                y_pop[idx] = y_offspring

        return pop, y_pop